{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a0cac8a-bc80-4f4c-805a-2aabf444e8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain-core langgraph>0.2.27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c20356a-94c7-4b91-938c-9e96413f28fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "os.environ[\"LANGSMITH_TRACING\"]=\"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"]=getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c476b0de-2c09-452a-9e61-58f63d3012b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU \"langchain[groq]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5f5bf09-1841-4c77-8716-5d111c85a3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "enter groq api key: ········\n"
     ]
    }
   ],
   "source": [
    "if not os.environ.get(\"GROQ_API_KEY\"):\n",
    "    os.environ[\"GROQ_API_KEY\"]=getpass.getpass(\"enter groq api key:\")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "model=init_chat_model(\"deepseek-r1-distill-llama-70b\",model_provider=\"groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df9d5dbd-561d-4cd9-a0c8-dc7d57746df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage,AIMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2730dad2-1d8f-4c9c-b857-2270e3b403f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\atul.yadav\\langchain-academy\\lc-academy-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"hi! I'm bob\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='I like vanilla ice cream', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='nice', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='whats 2 + 2', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='4', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='having fun?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='yes!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "\n",
    "trimmer =trim_messages(\n",
    "    max_tokens=65,\n",
    "    strategy=\"last\",\n",
    "    start_on=\"human\",\n",
    "    token_counter=model,\n",
    "    allow_partial=True,\n",
    "    include_system=True,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"hi! I'm bob\"),\n",
    "    AIMessage(content=\"hi!\"),\n",
    "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
    "    AIMessage(content=\"nice\"),\n",
    "    HumanMessage(content=\"whats 2 + 2\"),\n",
    "    AIMessage(content=\"4\"),\n",
    "    HumanMessage(content=\"thanks\"),\n",
    "    AIMessage(content=\"no problem!\"),\n",
    "    HumanMessage(content=\"having fun?\"),\n",
    "    AIMessage(content=\"yes!\"),\n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c51b1bf5-2069-425b-83fb-3575ef5bfcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt_temp=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"you are like a google , answer all question to best of your knowledge in short and concise manner in {language}.\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9c0367f-0999-456a-84e3-5e41c50ab013",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START,MessagesState,StateGraph\n",
    "from typing import Sequence \n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages:Annotated[Sequence[BaseMessage],add_messages]\n",
    "    language:str\n",
    "    \n",
    "workflow =StateGraph(state_schema=State)\n",
    "\n",
    "async def call_model(state:State):\n",
    "    trimmed_message=trimmer.invoke(state[\"messages\"])\n",
    "    \n",
    "    prompt=prompt_temp.invoke({\"messages\":trimmed_message,\"language\":state[\"language\"]})\n",
    "    response=await model.ainvoke(prompt)\n",
    "    return {\"messages\":[response]+trimmed_message}\n",
    "\n",
    "workflow.add_node(\"model\",call_model)\n",
    "workflow.add_edge(START,\"model\")\n",
    "\n",
    "memory=MemorySaver()\n",
    "chatbot=workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "139fe74a-5550-41ec-9bec-14c240257ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Alright, the user is asking, \"which match problem we talked about.\" Looking back at our conversation, I see that earlier they asked \"whats 2 + 2\" and I replied with \"4.\" So, they're referring to that math problem.\n",
      "\n",
      "They might be checking if I remember our previous interaction or perhaps they want to discuss more about it. I should acknowledge the specific problem we solved and offer further assistance in case they have more questions or need help with something else.\n",
      "\n",
      "I'll phrase my response to confirm that it was the 2 + 2 problem and let them know I'm here to help with anything else they need.\n",
      "</think>\n",
      "\n",
      "We talked about solving the math problem **2 + 2**, which equals **4**. Let me know if you need help with anything else!\n"
     ]
    }
   ],
   "source": [
    "query=\"which match problem we talked about\"\n",
    "language=\"english\"\n",
    "input_message=messages+[HumanMessage(query)]\n",
    "response = await chatbot.ainvoke({\"messages\":input_message,\"language\":language},config={\"configurable\":{\"thread_id\":\"7\"}})\n",
    "response[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78499269-eb47-41e9-9000-fa8952c6092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|<think>|\n",
      " previous| conversation|.| I| need| to| figure| out| what| they|'re| referring| to|. I| remember| they| mentioned| something| about| a| match| problem| before|,| but| I| don|'t| have| the| details| of| our|\n",
      "\n",
      " you| have| to| arrange| matches| to| form| certain| shapes| or| numbers|.| There| are| several| classic| problems|,| such| as| forming| a| square| or| a| triangle| with| a| certain| number| of| matches|.\n",
      "\n",
      " the| minimum| number| of| matches| needed| to| balance| an| equation| or| form| a| particular| shape|.| Another| possibility| is| that| they|'re| talking| about| a| problem| involving| burning| matches|,| like| the| classic| river| crossing| puzzle| or| something| related| to| time| measurement| with| burning| matches|.\n",
      "\n",
      " context| from| our| previous| conversation|,| I| should| ask| for| clarification|.| I|'ll| let| them| know| I| don|'t| recall| the| specifics| and| ask| them| to| provide| more| details| so| I| can| assist| them| better|.| That| way|,| I| can| give| a| more| accurate| and| helpful| response|.\n",
      "|</think>|\n",
      "\n",
      "|I| don|'t| recall| the| specifics| of| our| previous| conversation| about| the| match| problem|.| Could| you| please| provide| more| details| or| clarify| the| context|?| This| will| help| me| assist| you| more| effectively|.||==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Alright, the user is asking, \"which match problem we talked about.\" Looking back at our conversation, I see that earlier they asked \"whats 2 + 2\" and I replied with \"4.\" So, they're referring to that math problem.\n",
      "\n",
      "They might be checking if I remember our previous interaction or perhaps they want to discuss more about it. I should acknowledge the specific problem we solved and offer further assistance in case they have more questions or need help with something else.\n",
      "\n",
      "I'll phrase my response to confirm that it was the 2 + 2 problem and let them know I'm here to help with anything else they need.\n",
      "</think>\n",
      "\n",
      "We talked about solving the math problem **2 + 2**, which equals **4**. Let me know if you need help with anything else!\n"
     ]
    }
   ],
   "source": [
    "query=\"which match problem we talked about\"\n",
    "language=\"english\"\n",
    "input_message=[HumanMessage(query)]\n",
    "#response = await chatbot.ainvoke({\"messages\":input_message,\"language\":language},config={\"configurable\":{\"thread_id\":\"8\"}})\n",
    "\n",
    "\n",
    "async for chunk, metadata in chatbot.astream(\n",
    "    {\"messages\":input_message,\"language\":language},config={\"configurable\":{\"thread_id\":\"8\"}},stream_mode=\"messages\",\n",
    "):\n",
    "    if isinstance(chunk,AIMessage):\n",
    "        print(chunk.content,end=\"|\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5a35e03d-de32-4e92-869b-981f41a0ffc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, the user is asking me to act like Google and answer their questions concisely in Chinese. They're starting with \"What is my name.\" Hmm, I don't have access to personal information unless they provide it. I should respond politely that I can't tell them their name because I don't have that data. I need to make sure my answer is in Chinese, short, and clear. Maybe something like, \"我无法知道您的名字，因为我无法访问个人信息。\"\n",
      "</think>\n",
      "\n",
      "我无法知道您的名字，因为我无法访问个人信息。\n"
     ]
    }
   ],
   "source": [
    "query=\"What is my name\"\n",
    "language=\"Chinese\"\n",
    "input_message=[HumanMessage(query)]\n",
    "response = await chatbot.ainvoke({\"messages\":input_message,\"language\":language},config={\"configurable\":{\"thread_id\":\"4\"}})\n",
    "response[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1752c04c-ebed-4a51-9b11-4cb7b1c11e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Okay, the user is asking me to act like Google, answering questions concisely in Spanish. They introduced themselves as Chetan. I should acknowledge that and offer assistance in Spanish, keeping it friendly and open-ended.\n",
      "</think>\n",
      "\n",
      "¡Hola Chetan! ¿En qué puedo ayudarte hoy?\n"
     ]
    }
   ],
   "source": [
    "query=\"I m Chetan\"\n",
    "language=\"Spanish\"\n",
    "input_message=[HumanMessage(query)]\n",
    "response = await chatbot.ainvoke({\"messages\":input_message,\"language\":language},config={\"configurable\":{\"thread_id\":\"3\"}})\n",
    "response[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "49b28d1c-d52c-41e4-bcd1-75a5246dcbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<think>\n",
      "Alright, the user has just asked, \"WHAT IS MY NAME.\" Looking back at the history, they introduced themselves as Chetan. So, I should respond with their name. Since they want answers in Spanish, I'll say \"Tu nombre es Chetan.\" That's straightforward and meets their request.\n",
      "</think>\n",
      "\n",
      "Tu nombre es Chetan.\n"
     ]
    }
   ],
   "source": [
    "#NOW ENTRIE STATE IS PERSISTATE SO WE OMITTED THE LANGUAGE\n",
    "query=\"WHAT IS MY NAME\"\n",
    "input_message=[HumanMessage(query)]\n",
    "response = await chatbot.ainvoke({\"messages\":input_message},config={\"configurable\":{\"thread_id\":\"3\"}})\n",
    "response[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9c733f-3625-4117-aa2f-691910aba1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
